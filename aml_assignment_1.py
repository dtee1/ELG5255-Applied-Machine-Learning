# -*- coding: utf-8 -*-
"""AML_Assignment 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BmD_EQKB8ApE0xA9a5L8J5DN648AovVh

# Question 1(a) :

References:

https://www.mygreatlearning.com/blog/label-encoding-in-python/ 

https://www.adamsmith.haus/python/answers/how-to-read-specific-column-from-csv-file-in-python#:~:text=Use%20pandas.,read%20from%20the%20CSV%20file.
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder

data_train = pd.read_csv("DUMD_train.csv")

data_train

labelencoder = LabelEncoder()

data_train["UNS_N"] = labelencoder.fit_transform(data_train["UNS"])

data_train.to_csv(r'D:\Applied Machine Learning\DUMD_train_New.csv', index = False)

data_train

data_test = pd.read_csv("DUMD_test.csv")


data_test

data_test["UNS_N"] = labelencoder.fit_transform(data_test["UNS"])

data_test.to_csv(r'D:\Applied Machine Learning\DUMD_test_New.csv', index = False)

data_test

"""# Question 1(b) :

References:

https://vitalflux.com/python-scatter-plot-different-classes/
"""

import pandas as pd
import matplotlib.pyplot as plt
 
data_plot = pd.read_csv('/content/DUMD_train_New.csv')
data_plot.head()

plt.scatter(data_plot['LPR'][ data_plot.UNS_N == 0],
            data_plot['PEG'][ data_plot.UNS_N == 0],
           marker='.',
           color='red',
           label='High')
plt.scatter(data_plot['LPR'][data_plot.UNS_N == 1],
            data_plot['PEG'][data_plot.UNS_N == 1],
           marker='.',
           color='green',
           label='Low')
plt.scatter(data_plot['LPR'][data_plot.UNS_N == 2],
            data_plot['PEG'][data_plot.UNS_N == 2],
           marker='.',
           color='blue',
           label='Medium')
plt.scatter(data_plot['LPR'][data_plot.UNS_N == 3],
            data_plot['PEG'][data_plot.UNS_N == 3],
           marker='.',
           color='orange',
           label='Very Low')
plt.xlabel('LPR')
plt.ylabel('PEG')
plt.legend()
plt.show()

"""# Question 1(c):

References:
   
https://pub.towardsai.net/support-vector-machine-svm-for-binary-and-multiclass-classification-hands-on-with-scikit-learn-29cdbe5cb90e

https://www.quantstart.com/articles/training-the-perceptron-with-scikit-learn-and-tensorflow/

https://chrisalbon.com/code/machine_learning/basics/perceptron_in_scikit-learn/

### SVM
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

svm_train_data = pd.read_csv("/content/DUMD_train_New.csv")
svm_train_data.head()

svm_test_data = pd.read_csv("/content/DUMD_test_New.csv")
svm_test_data.head()

svm_train_data['UNS_N'].value_counts()

high_df = svm_train_data[svm_train_data['UNS_N']==0][0:200]
low_df = svm_train_data[svm_train_data['UNS_N']==1][0:200]
medium_df = svm_train_data[svm_train_data['UNS_N']==2][0:200]
verylow_df = svm_train_data[svm_train_data['UNS_N']==3][0:200]

axes = high_df.plot(kind='scatter', x = 'LPR', y = 'PEG', color='red', label='high')
low_df.plot(kind='scatter', x = 'LPR', y = 'PEG', color='green', label='low', ax = axes)
medium_df.plot(kind='scatter', x = 'LPR', y = 'PEG', color='yellow', label='medium', ax = axes)
verylow_df.plot(kind='scatter', x = 'LPR', y = 'PEG', color='blue', label='verylow', ax = axes)

feature_df_train = svm_train_data[['LPR', 'PEG']]

svm_train_x = np.array(feature_df_train)
svm_train_y = np.array(svm_train_data['UNS_N'])

feature_df_test = svm_test_data[['LPR', 'PEG']]

svm_test_x = np.array(feature_df_test)
svm_test_y = np.array(svm_test_data['UNS_N'])

from sklearn import svm

classifier = svm.SVC(kernel='linear', C=100)
classifier.fit(svm_train_x, svm_train_y)
svm_y_predict = classifier.predict(svm_test_x)

from sklearn.metrics import classification_report, ConfusionMatrixDisplay, accuracy_score, confusion_matrix
import seaborn as sns # for plotting.

print(classification_report(svm_test_y, svm_y_predict))
# print(confusion_matrix(svm_test_y, svm_y_predict))

ax= plt.subplot()
# predict_results = model.predict(normed_test_data)

cm = confusion_matrix(svm_test_y, svm_y_predict)

sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
ax.set_title('Confusion Matrix'); 
# ax.xaxis.set_ticklabels(['Positive', 'Negative']); ax.yaxis.set_ticklabels(['Positive', 'Negative']);

def getAccuracy(model, x, y):
    return model.score(x,y)*100

print('Accuracy of model: {:.2f}%'.format(getAccuracy(classifier, svm_test_x, svm_test_y)))

from mlxtend.plotting import plot_decision_regions

#0 = high
#1 = low
#2 = medium
#3 = verylow

plot_decision_regions(svm_test_x, svm_test_y, classifier)

"""### Perceptron"""

from sklearn.linear_model import Perceptron
p = Perceptron(random_state=1)
p.fit(svm_train_x, svm_train_y)

per_y_predict = p.predict(svm_test_x)

print(classification_report(svm_test_y, per_y_predict))
# print(confusion_matrix(svm_test_y, per_y_predict))

axp = plt.subplot()
# predict_results = model.predict(normed_test_data)

cm_p = confusion_matrix(svm_test_y, per_y_predict)

sns.heatmap(cm_p, annot=True, ax = axp); #annot=True to annotate cells

# labels, title and ticks
axp.set_xlabel('Predicted labels');axp.set_ylabel('True labels'); 
axp.set_title('Confusion Matrix Perceptron'); 
# ax.xaxis.set_ticklabels(['Positive', 'Negative']); ax.yaxis.set_ticklabels(['Positive', 'Negative']);

print('Accuracy of model: {:.2f}%'.format(getAccuracy(p, svm_test_x, svm_test_y)))

from mlxtend.plotting import plot_decision_regions
plot_decision_regions(svm_test_x, svm_test_y, p)

#0 = high
#1 = low
#2 = medium
#3 = verylow

"""# Question 2(a):"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.preprocessing import MultiLabelBinarizer

svm_train_data = pd.read_csv("/content/DUMD_train_New.csv")
svm_test_data = pd.read_csv("/content/DUMD_test_New.csv")

feature_df_train = svm_train_data[['LPR', 'PEG']]

svm_train_x = np.array(feature_df_train)
svm_train_y = np.array(svm_train_data['UNS_N'])

feature_df_test = svm_test_data[['LPR', 'PEG']]

svm_test_x = np.array(feature_df_test)
svm_test_y = np.array(svm_test_data['UNS_N'])

svm_test_y

yb = svm_test_y.reshape((-1,1))
mlb = MultiLabelBinarizer()
yb = mlb.fit_transform(yb)

yb1 = yb[:,0] #class=high
yb2 = yb[:,1] #class=low
yb3 = yb[:,2] #class=medium
yb4 = yb[:,3] #class=verylow

"""## class = high binary classifier


"""

#binarized labels 
yb1

#SVM's Accuracy
clf_1 = svm.SVC(kernel='rbf', probability=True)
clf_1.fit(svm_test_x, yb1)
print('Accuracy of clf_1: {:.2f}%'.format(getAccuracy(clf_1, svm_test_x, yb1)))
yb1_pred = clf_1.predict_proba(svm_test_x)[:,1].reshape(-1,1)

#Decision Boundary
plot_decision_regions(svm_test_x, yb1, clf_1)

"""## class = low binary classifier"""

#binarized labels 
yb2

#class - low binary classifier
clf_2 = svm.SVC(kernel='rbf', probability=True)
clf_2.fit(svm_test_x, yb2)
print('Accuracy of clf_2: {:.2f}%'.format(getAccuracy(clf_2, svm_test_x, yb2)))
yb2_pred = clf_2.predict_proba(svm_test_x)[:,1].reshape(-1,1)

#Decision Boundary
plot_decision_regions(svm_test_x, yb2, clf_2)

"""## class = medium binary classifier"""

#binarized labels 
yb3

#class - medium binary classifier
clf_3 = svm.SVC(kernel='rbf', probability=True)
clf_3.fit(svm_test_x, yb3)
print('Accuracy of clf_3: {:.2f}%'.format(getAccuracy(clf_3, svm_test_x, yb3)))
yb3_pred = clf_3.predict_proba(svm_test_x)[:,1].reshape(-1,1)

#Decision Boundary
plot_decision_regions(svm_test_x, yb3, clf_3)

"""## class = verylow binary classifier"""

#binarized labels 
yb4

#class - verylow binary classifier
clf_4 = svm.SVC(kernel='rbf', probability=True)
clf_4.fit(svm_test_x, yb4)
print('Accuracy of clf_4: {:.2f}%'.format(getAccuracy(clf_4, svm_test_x, yb4)))
yb4_pred = clf_4.predict_proba(svm_test_x)[:,1].reshape(-1,1)

#Decision Boundary
plot_decision_regions(svm_test_x, yb4, clf_4)

"""# Question 2(b):"""

yb_all = np.hstack((yb1_pred, yb2_pred, yb3_pred, yb4_pred))
m = mlb.classes_[np.argmax(yb_all, axis=1)]

m = m.astype(str).astype(int)

print(classification_report(svm_test_y, m))
# print(confusion_matrix(svm_test_y, svm_y_predict))

ax_ovr = plt.subplot()
# predict_results = model.predict(normed_test_data)

cm_ovr = confusion_matrix(svm_test_y, m)

sns.heatmap(cm_ovr, annot=True, ax = ax_ovr); #annot=True to annotate cells

# labels, title and ticks
ax_ovr.set_xlabel('Predicted labels');ax_ovr.set_ylabel('True labels'); 
ax_ovr.set_title('Confusion Matrix'); 
# ax.xaxis.set_ticklabels(['Positive', 'Negative']); ax.yaxis.set_ticklabels(['Positive', 'Negative']);

axis = plot_decision_regions(svm_test_x, m, clf_1, legend=1)
plot_decision_regions(svm_test_x, m, clf_2, ax = axis, legend=0)
plot_decision_regions(svm_test_x, m, clf_3, ax = axis, legend=0)
plot_decision_regions(svm_test_x, m, clf_4, ax = axis, legend=0)

"""# Question 3(a):"""

# libraries
import numpy as np # used for handling numbers
import pandas as pd # used for handling the dataset
from sklearn.impute import SimpleImputer # used for handling missing data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder # used for encoding categorical data
from sklearn.model_selection import train_test_split # used for splitting training and testing data
from sklearn.preprocessing import StandardScaler # used for feature scaling

knn_data = pd.read_csv("/content/car_evaluation_new.csv")

knn_data.head()

feature_df = knn_data[['buying price', 'maintenance cost','number of doors', 'number of persons','lug_boot', 'safety']]

knn_x = np.array(feature_df)
knn_y = np.array(knn_data['class'])

# We want to split the data in 57.87:17.36:24.77 for train:valid:test dataset
train_size=0.5788

X = knn_data.drop(columns = ['class']).copy()
y = knn_data['class']

# In the first step we will split the data in training and remaining dataset
X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.5788)

# Now since we want the valid and test size to be 17.36:24.77. 
# we have to define valid_size=0.4121 (that is 41.21% of remaining data)
test_size = 0.5879
X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5879)

print(X_train.shape), print(y_train.shape)
print(X_valid.shape), print(y_valid.shape)
print(X_test.shape), print(y_test.shape)

"""# Question 3(b):"""

knn_data.info()

labelencoder = LabelEncoder()

knn_data['buying price'] = labelencoder.fit_transform(knn_data['buying price'])
knn_data['maintenance cost'] = labelencoder.fit_transform(knn_data['maintenance cost'])
knn_data['number of doors'] = labelencoder.fit_transform(knn_data['number of doors'])
knn_data['lug_boot'] = labelencoder.fit_transform(knn_data['lug_boot'])
knn_data['safety'] = labelencoder.fit_transform(knn_data['safety'])
knn_data['class'] = labelencoder.fit_transform(knn_data['class'])

knn_data.to_csv(r'Car_Evaluation_New.csv', index = False)
knn_data_new = pd.read_csv("/content/Car_Evaluation_data.csv")
knn_data_new.info()

"""# Question 3(c):"""

knn_data_new = pd.read_csv("/content/Car_Evaluation_data.csv")
knn_data_new.info()

feature_df_new = knn_data_new[['buying price', 'maintenance cost','number of doors', 'number of persons','lug_boot', 'safety']]

knn_x_new = np.array(feature_df_new)
knn_y_new = np.array(knn_data_new['class'])

# We want to split the data in 57.87:17.36:24.77 for train:valid:test dataset
train_size=0.5788

X_new = knn_data_new.drop(columns = ['class']).copy()
y_new = knn_data_new['class']

# In the first step we will split the data in training and remaining dataset
X_train, X_rem, y_train, y_rem = train_test_split(X_new,y_new, train_size=0.5788)

# Now since we want the valid and test size to be 17.36:24.77. 
# we have to define valid_size=0.4121 (that is 41.21% of remaining data)
test_size = 0.5879
X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5879)

print(X_train.shape), print(y_train.shape)
print(X_valid.shape), print(y_valid.shape)
print(X_test.shape), print(y_test.shape)

samples = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]
mean_acc_valid = np.zeros((10))
std_acc_valid = np.zeros((10))
mean_acc_test = np.zeros((10))
std_acc_test = np.zeros((10))
ConfustionMx = [];
for n in range(0,len(samples)):
    
    #Train Model and Predict  
    x = X_train
    y = y_train
    X_train_loop, X_rem, y_train_loop, y_rem = train_test_split(x,y, train_size=samples[n])
    neigh = KNeighborsClassifier(n_neighbors = 2).fit(X_train_loop,y_train_loop)
    yhat_valid=neigh.predict(X_valid)
    mean_acc_valid[n] = metrics.accuracy_score(y_valid, yhat_valid)
    std_acc_valid[n]=np.std(yhat_valid==y_valid)/np.sqrt(yhat_valid.shape[0])

    yhat_test=neigh.predict(X_test)
    mean_acc_test[n] = metrics.accuracy_score(y_test, yhat_test)
    std_acc_test[n]=np.std(yhat_test==y_test)/np.sqrt(yhat_test.shape[0])

neigh = KNeighborsClassifier(n_neighbors = 2).fit(X_train,y_train)
yhat_valid=neigh.predict(X_valid)
mean_acc_valid[9] = metrics.accuracy_score(y_valid, yhat_valid)
std_acc_valid[9]=np.std(yhat_valid==y_valid)/np.sqrt(yhat_valid.shape[0])

yhat_test=neigh.predict(X_test)
mean_acc_test[9] = metrics.accuracy_score(y_test, yhat_test)
std_acc_test[9]=np.std(yhat_test==y_test)/np.sqrt(yhat_test.shape[0])

plt.plot(range(0,10),mean_acc_test,'g')
plt.plot(range(0,10),mean_acc_valid,'r')
# plt.fill_between(range(0,10),mean_acc_test - 1 * std_acc_test,mean_acc_test + 1 * std_acc_test, alpha=0.10)
# plt.fill_between(range(0,10),mean_acc_valid - 1 * std_acc_valid,mean_acc_valid + 1 * std_acc_valid, alpha=0.10)
plt.legend(('Test', 'Valid'))
plt.ylabel('Accuracy ')
plt.xlabel('Number of Samples in Percentage')
plt.show()

"""# Question 3(d):"""

# Commented out IPython magic to ensure Python compatibility.
import itertools
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import NullFormatter
import pandas as pd
import numpy as np
import matplotlib.ticker as ticker
from sklearn import preprocessing
from sklearn import datasets, neighbors
from sklearn.model_selection import train_test_split
from mlxtend.plotting import plot_decision_regions
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
# %matplotlib inline

knn_data_new = pd.read_csv("/content/Car_Evaluation_data.csv")
knn_data_new.info()

feature_df_new = knn_data_new[['buying price', 'maintenance cost','number of doors', 'number of persons','lug_boot', 'safety']]

knn_x_new = np.array(feature_df_new)
knn_y_new = np.array(knn_data_new['class'])

# We want to split the data in 57.87:17.36:24.77 for train:valid:test dataset
train_size=0.5788

X_new = knn_data_new.drop(columns = ['class']).copy()
y_new = knn_data_new['class']

# In the first step we will split the data in training and remaining dataset
X_train, X_rem, y_train, y_rem = train_test_split(X_new,y_new, train_size=0.5788)

# Now since we want the valid and test size to be 17.36:24.77. 
# we have to define valid_size=0.4121 (that is 41.21% of remaining data)
test_size = 0.5879
X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5879)

print(X_train.shape), print(y_train.shape)
print(X_valid.shape), print(y_valid.shape)
print(X_test.shape), print(y_test.shape)

Ks = 10
mean_acc = np.zeros((Ks-1))
std_acc = np.zeros((Ks-1))
ConfustionMx = [];
for n in range(1,Ks):
    
    #Train Model and Predict  
    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)
    yhat=neigh.predict(X_valid)
    mean_acc[n-1] = metrics.accuracy_score(y_valid, yhat)

    
    std_acc[n-1]=np.std(yhat==y_valid)/np.sqrt(yhat.shape[0])

mean_acc

plt.plot(range(1,Ks),mean_acc,'g')
plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)
plt.legend(('Accuracy ', '+/- 3xstd'))
plt.ylabel('Accuracy ')
plt.xlabel('Number of Neignbours (K)')
plt.tight_layout()
plt.show()

print( "The best accuracy was with", mean_acc.max(), "with k=", mean_acc.argmax()+1)